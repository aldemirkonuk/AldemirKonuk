{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Cats', 'Dogs']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 1. Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 2. Create a dataset from the folder\n",
    "dataset = ImageFolder(root=\"Dataset\", transform=transform)\n",
    "print(\"Classes:\", dataset.classes)  # Should print ['Cats', 'Dogs']\n",
    "\n",
    "# 3. (Optional) Split into train and val sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size   = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is very effective for image classification \n",
    "because it automatically learns to extract important features (or \"embeddings\") from images—such as edges, textures, and shapes—through convolutional layers.\n",
    "\n",
    "This is the standard way to define a neural network in PyTorch. inherits from nn.Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layers\n",
    "\n",
    "Flattening the output:\n",
    "After three rounds of pooling on an image of size 128x128:\n",
    "128 → 64 (after conv1 + pool)\n",
    "64 → 32 (after conv2 + pool)\n",
    "32 → 16 (after conv3 + pool)\n",
    "The feature maps from the last convolution have dimensions 64 channels × 16 × 16.\n",
    "self.fc1 = nn.Linear(64 * 16 * 16, 128) creates a fully connected layer that takes this flattened vector as input and outputs 128 features.\n",
    "Output Layer:\n",
    "self.fc2 = nn.Linear(128, 2) maps the 128 features to 2 output classes (Cats and Dogs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layers: Learn features from images.\n",
    "\n",
    "Pooling: Reduces image dimensions and helps capture important features.\n",
    "\n",
    "Flattening: Prepares data for the fully connected layers.\n",
    "\n",
    "Fully Connected Layers: Make the final prediction (classifying into two classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=16384, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__() #This line calls the initializer of the parent class (nn.Module), which is necessary for PyTorch to manage your model correctly.\n",
    "        # 1) Convolutional layers\n",
    "        # Input channels: 3 (for RGB images).\n",
    "        # Output channels: 16 (the number of filters that will learn features).\n",
    "        # Kernel size: 3 (each filter is a 3x3 matrix).\n",
    "        # Padding: 1 (keeps the spatial dimensions the same before pooling).\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 2) Fully connected layers\n",
    "        # After 3 poolings on a 128x128 input, feature map is 16x16\n",
    "        self.fc1   = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2   = nn.Linear(128, 2)  # 2 classes: Cats and Dogs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 128 -> 64\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 64  -> 32\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 32  -> 16\n",
    "        x = x.view(-1, 64 * 16 * 16)          # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [2/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [3/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [4/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [5/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [6/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [7/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [8/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [9/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n",
      "Epoch [10/10], Loss: 0.6098\n",
      "Validation Accuracy: 57.27%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Start small for testing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # 1) Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # 2) Forward pass\n",
    "        outputs = model(images)\n",
    "        # 3) Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 4) Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cats_vs_dogs_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(\"cats_vs_dogs_model.pth\"))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
